{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as plt_colors\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from hybridhmm import HybridHMMTransitions, heuristic_priors_adjustment\n",
    "from models import SkelFeatureExtractor\n",
    "from utils import plot_transition_matrix\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "matplotlib.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True (synthetic) distribution\n",
    "\n",
    "We assume a Laplace-HMM distribution with 6 states such than $p(\\boldsymbol{x}_t \\mid s_t = i) = \\mathcal{L}(\\boldsymbol{x}_t; \\boldsymbol{\\mu}_i, 0.2)$ where $\\boldsymbol{\\mu}_i = \\left( cos \\left( \\frac{2 (i - 1) \\pi}{6} \\right), sin \\left( \\frac{2 (i - 1) \\pi}{6} \\right) \\right)$\n",
    "\n",
    "The duration of the sequences follows a poisson distribution of mean $\\lambda = 25$.\n",
    "\n",
    "Finally, the system always starts in state 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 5\n",
    "input_size = 2\n",
    "\n",
    "mu = np.array([\n",
    "    [np.cos(2 * np.pi * i / n_states), np.sin(2 * np.pi * i / n_states)]\n",
    "    for i in range(n_states)])\n",
    "\n",
    "true_transitions = [\n",
    "    [.96, .04, .00, .00, .00],\n",
    "    [.00, .80, .20, .00, .00],\n",
    "    [.00, .15, .70, .15, .00],\n",
    "    [.00, .00, .00, .85, .15],\n",
    "    [.15, .00, .00, .00, .85]]\n",
    "\n",
    "true_init_state_priors = [1.] + [0] * (len(true_transitions) - 1)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_transition_matrix(true_transitions)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(mu[:, 0], mu[:, 1])\n",
    "plt.axis('equal')\n",
    "plt.title(\"center points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Generate (noisy) data from the true model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_y = []\n",
    "\n",
    "for _ in range(100):\n",
    "    state_seq = []\n",
    "    state = np.random.choice(\n",
    "        np.arange(n_states), 1, \n",
    "        p=true_init_state_priors)[0]\n",
    "    state_seq.append(state)\n",
    "    \n",
    "    for _ in range(max(0, np.random.poisson(75) - 1)):\n",
    "        state = np.random.choice(\n",
    "            np.arange(n_states), 1, \n",
    "            p=true_transitions[state])[0]\n",
    "        state_seq.append(state)\n",
    "        if state == n_states - 1 and np.random.rand() > .87:\n",
    "            break\n",
    "    \n",
    "    dataset_y.append(np.array(state_seq))\n",
    "\n",
    "dataset_x = []\n",
    "\n",
    "for y in dataset_y:\n",
    "    dataset_x.append(mu[y] + np.random.laplace(scale=.2, size=(len(y), input_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the noise on the datapoints will generate observations that are difficult to attribute to their corresponding state.\n",
    "The transition model of the HMM will hopefully lift the ambiguity for most of these situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_observations = np.concatenate(dataset_x)\n",
    "all_labels = np.concatenate(dataset_y)\n",
    "for i in range(n_states):\n",
    "    plt.scatter(all_observations[all_labels == i, 0], \n",
    "                all_observations[all_labels == i, 1],\n",
    "                s=5)\n",
    "    \n",
    "plt.axis('equal');\n",
    "plt.title('Observation data points')\n",
    "plt.legend([str('$s={}$'.format(i)) for i in range(n_states)],\n",
    "           title=\"underlying state\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Below, we create the model and training routines.\n",
    "The transition matrix encodes several assumptions:\n",
    "\n",
    "- Our system describes certain events which involve 4 states.\n",
    "- We have little information about the order of these states, except that the event always begins with the same state and ends in another one\n",
    "- Inside the event, we do not know if the states appear in a certain order or not, except that we cannot loop back from the last to the first state.\n",
    "- Before and after an event occurs, the system stays in a separate 'resting' state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = [\n",
    "    [.90, .04, .00, .00, .00],\n",
    "    [.00, .70, .10, .10, .10],\n",
    "    [.00, .10, .70, .10, .10],\n",
    "    [.00, .10, .10, .70, .10],\n",
    "    [.10, .00, .10, .10, .70]]\n",
    "init_state_priors = [1.] + [0] * (len(true_transitions) - 1)\n",
    "state_priors = np.full([n_states], 1 / n_states, dtype='float32')\n",
    "\n",
    "plot_transition_matrix(transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code to create the model parts, everything is implemented with Tensorflow (no wrapper of dependency used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Models and inference\n",
    "\n",
    "training = tf.placeholder(tf.bool, shape=[], name='training')\n",
    "inputs = tf.placeholder(dtype='float32', shape=[None, input_size], name='inputs')\n",
    "\n",
    "net = SkelFeatureExtractor(n_states)\n",
    "\n",
    "logits = net(inputs, training)\n",
    "state_posteriors = tf.nn.log_softmax(logits)\n",
    "\n",
    "hmm = HybridHMMTransitions(transitions, init_state_priors, state_priors)\n",
    "\n",
    "pseudo_lkh = hmm.pseudo_lkh(state_posteriors)\n",
    "ml_state_alignment = hmm.viterbi(pseudo_lkh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then comes the training routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "state_tgt = tf.placeholder(dtype='int64', shape=[None], name='state_tgt')\n",
    "loss_weights = tf.placeholder(dtype='float32', shape=[n_states], name='loss_weight')\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=state_tgt,\n",
    "        logits=logits) * tf.gather(loss_weights, state_tgt))\n",
    "\n",
    "learning_rate = tf.placeholder(dtype='float32', shape=[], name='learning_rate')\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)  # batch norm updates\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    optimization_op = optimizer.minimize(loss, var_list=net.trainable_variables)\n",
    "\n",
    "add_transition_stats_op = hmm.add_transition_stats(pseudo_lkh)\n",
    "update_transitions_op = hmm.update_transitions()\n",
    "add_prior_stats_op = hmm.add_priors_stats(pseudo_lkh)\n",
    "update_state_priors = hmm.update_state_priors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(more Tensorflow boilerplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sess' in globals():\n",
    "    sess.close()\n",
    "    \n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training procedure\n",
    "\n",
    "## First training pass\n",
    "\n",
    "Training begins with the posterior state model, that is to say the Neural Network.\n",
    "In order to proceed, target values are needed.\n",
    "In this case, the targets are the state values which are unfortunately unobserved.\n",
    "To circumvent this problem, we initially select suboptimal values chosen arbitrarily according to an heuristic adapted to the use-case, for example the result of a GMM-HMM model or a k-means clustering.\n",
    "\n",
    "Once all model parts have been trained at least once, the model is assumed to be good enough to provide sensible state values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(data, n):\n",
    "    return np.sum(np.asarray(data)[:, None] == np.arange(n)[None, :], axis=0)\n",
    "\n",
    "# we use a k-Means to generate initial state values\n",
    "kmeans = KMeans(n_clusters=n_states)\n",
    "kmeans.fit(all_observations)\n",
    "kmeans_state_predictions = [kmeans.predict(x) for x in dataset_x]\n",
    "\n",
    "# to refine the initialization, we will use our asumption that states mostly appear in succession\n",
    "centers2state = []\n",
    "\n",
    "# we know the system always starts in state 0\n",
    "centers2state.append(\n",
    "    np.argmax(count([p[0] for p in kmeans_state_predictions], n_states)))\n",
    "\n",
    "# iteratively find the next state\n",
    "for _ in range(n_states - 1):\n",
    "    successors = []\n",
    "    for p in kmeans_state_predictions:\n",
    "        is_successor = np.invert(np.isin(p[1:], centers2state)) & (p[:-1] == centers2state[-1])\n",
    "        successors.extend(p[1:][is_successor])\n",
    "    \n",
    "    ml_successor = np.argmax(count(successors, n_states))\n",
    "    centers2state.append(ml_successor)\n",
    "\n",
    "# reorder clusters to (probably) match states\n",
    "kmeans.cluster_centers_ = kmeans.cluster_centers_[centers2state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this simple example with synthetic data, kmeans provides a strong initialization.\n",
    "When working with real data, it can become difficult to provide sensible initialization values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_states):\n",
    "    plt.scatter(\n",
    "        [kmeans.cluster_centers_[i, 0], mu[i, 0]],\n",
    "        [kmeans.cluster_centers_[i, 1], mu[i, 1]])\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.legend([str(i) for i in range(n_states)])\n",
    "plt.title(\"k-Mean centers vs true state distributions means\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the kmeans heuristic to set state posterior targets\n",
    "\n",
    "state_alignments = [\n",
    "    kmeans.predict(x)\n",
    "    for x in dataset_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, a bad initialization of state targets might easily send the training procedure on a degenerative and non-recoverable path.\n",
    "Supervision of the training procedure is critical and expertise is required to make sure the state alignment does not ignore states or otherwise attribute too many observations to a single states.\n",
    "\n",
    "This can be monitored by checking how often each state is visited.\n",
    "\n",
    "Some imbalance is not abnormal, some states may naturally appear more often, but imbalance will quickly detoriorate the quality of the posterior state model (the neural network): it will overly focus on the most frequent states while ignoring the others.\n",
    "To counter this effect, we could resample the dataset but we choose to reweight the loss depending on the label, which has the same effect on average but is simpler to implement.\n",
    "\n",
    "Check the usage of the `loss_weights` variable in the expression of the loss above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = count(np.concatenate(state_alignments), n_states)\n",
    "freqs = freqs / np.sum(freqs)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(np.arange(hmm.n_states), count(np.concatenate(state_alignments), n_states))\n",
    "plt.title(\"state counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state posterior model training iterations\n",
    "net_losses = []\n",
    "for _ in range(100):\n",
    "    i = np.random.randint(len(dataset_x))\n",
    "    loss_value, _ = sess.run(\n",
    "        [loss, optimization_op],\n",
    "        feed_dict={\n",
    "            inputs: dataset_x[i],\n",
    "            state_tgt: state_alignments[i],\n",
    "            loss_weights: n_states / freqs,\n",
    "            learning_rate: 0.001,\n",
    "            training: True\n",
    "        })\n",
    "    net_losses.append(loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the posterior model, it is possible to train the transition model.\n",
    "The accumulated statistics from all the observation sequences are used to update the initial state prior $\\pi_i = p(s_1=i)$ and the transition probabilities $a_{ij} = p(s_{t+1}=j \\mid s_t=i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dataset_x:\n",
    "    sess.run(\n",
    "        [add_transition_stats_op],\n",
    "        feed_dict={\n",
    "            inputs: x,\n",
    "            training: False\n",
    "        })\n",
    "\n",
    "sess.run(update_transitions_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our neural network is unbiased with respect to state frequencies (reweighting mechanism), the state priors remain uniform.\n",
    "The output of the Neural Network is assumed to be proportional to the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate state priors from the state frequencies observed on the training dataset with the current model\n",
    "\n",
    "# for x in dataset_x:\n",
    "#     sess.run(\n",
    "#         add_prior_stats_op,\n",
    "#         feed_dict={\n",
    "#             inputs: x,\n",
    "#             training: False\n",
    "#         })\n",
    "# \n",
    "# sess.run(update_state_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All model parts have been trained once now. Let's recaptulate the training process thus far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 2), dpi=100)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(range(len(net_losses)), net_losses, c='red', s=5, alpha=.3)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.title(\"CE loss\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plot_transition_matrix(np.exp(sess.run(hmm.A)))\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(np.arange(hmm.n_states), np.exp(sess.run(hmm.state_priors)))\n",
    "plt.title(\"state priors\")\n",
    "plt.ylim((0.01, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are successive refinement iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(5):    \n",
    "    # realign states\n",
    "    state_alignments = [\n",
    "        sess.run(ml_state_alignment, feed_dict={inputs: x, training: False})\n",
    "        for x in dataset_x]\n",
    "    \n",
    "    freqs = count(np.concatenate(state_alignments), n_states)\n",
    "    freqs = freqs / np.sum(freqs)\n",
    "\n",
    "    # state posterior model (Neural Network)\n",
    "    for _ in range(100):\n",
    "        i = np.random.randint(len(dataset_x))\n",
    "        loss_value, _ = sess.run(\n",
    "            [loss, optimization_op],\n",
    "            feed_dict={\n",
    "                inputs: dataset_x[i],\n",
    "                state_tgt: state_alignments[i],\n",
    "                loss_weights: n_states / freqs,\n",
    "                learning_rate: 0.001,\n",
    "                training: True\n",
    "            })\n",
    "        net_losses.append(loss_value)\n",
    "    \n",
    "    # transition model\n",
    "    for x in dataset_x:\n",
    "        sess.run(\n",
    "            add_transition_stats_op,\n",
    "            feed_dict={\n",
    "                inputs: x,\n",
    "                training: False\n",
    "            })\n",
    "\n",
    "    sess.run(update_transitions_op)\n",
    "    \n",
    "    # state priors\n",
    "    #for x in dataset_x:\n",
    "    #    sess.run(\n",
    "    #        add_prior_stats_op,\n",
    "    #        feed_dict={\n",
    "    #            inputs: x,\n",
    "    #            training: False\n",
    "    #        })\n",
    "    #\n",
    "    #sess.run(update_state_priors)\n",
    "    \n",
    "    plt.figure(figsize=(9, 2), dpi=100)\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.scatter(np.arange(len(net_losses) - 100), net_losses[:-100], c='gray', s=5, alpha=.3)\n",
    "    plt.scatter(np.arange(len(net_losses) - 100, len(net_losses)), net_losses[-100:], c='red', s=5, alpha=.3)\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.title(\"CE loss\")\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plot_transition_matrix(np.exp(sess.run(hmm.A)))\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.bar(np.arange(hmm.n_states), np.exp(sess.run(hmm.state_priors)))\n",
    "    plt.title(\"state priors\")\n",
    "    plt.ylim((0.01, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Below are some observations of the model in action.\n",
    "The output of the Neural Network (logits) is sometime mistaken on what state is active at a given time, but the HMM transition model (viterbi) brings temporal coherence and eliminates these errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    p, a = sess.run(\n",
    "        [tf.argmax(logits, axis=1),  # most likely state according to the posterior state model\n",
    "         ml_state_alignment],  # most likely states according to the HMM (viterbi)\n",
    "        feed_dict={\n",
    "            inputs: dataset_x[i],\n",
    "            training: False\n",
    "        })\n",
    "    print(\"logits:   \" + ''.join(map(str, p)))\n",
    "    print(\"viterbi:  \" + ''.join(map(str, a)))\n",
    "    print(\"gndtruth: \" + ''.join(map(str, dataset_y[i])))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
