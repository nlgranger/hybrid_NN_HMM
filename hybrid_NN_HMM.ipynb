{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as plt_colors\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from hybridhmm import HybridHMMTransitions, heuristic_priors_uniform\n",
    "from models import SkelFeatureExtractor\n",
    "from utils import plot_transition_matrix\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "matplotlib.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True (synthetic) distribution\n",
    "\n",
    "We assume a Gaussian-HMM distribution. That is to say $p(\\boldsymbol{x}_t \\mid s_t = i) = \\mathcal{N}(\\boldsymbol{x}_t; \\boldsymbol{\\mu}_i, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 16\n",
    "n_states = 6\n",
    "\n",
    "mu = np.random.randn(n_states, input_size) * 4\n",
    "\n",
    "true_transitions = [\n",
    "    [.82, .18, .00, .00, .00, .00],\n",
    "    [.05, .80, .15, .00, .00, .00],\n",
    "    [.00, .07, .90, .03, .00, .00],\n",
    "    [.00, .00, .00, .95, .05, .00],\n",
    "    [.00, .00, .00, .01, .95, .04],\n",
    "    [.00, .00, .00, .00, .15, .85]]\n",
    "plot_transition_matrix(true_transitions)\n",
    "\n",
    "true_init_state_priors = np.array([1.] + [0] * (n_states - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Generate (noisy) data from the true model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_y = []\n",
    "\n",
    "for _ in range(100):\n",
    "    state_seq = []\n",
    "    state = np.random.choice(\n",
    "        np.arange(n_states), 1, \n",
    "        p=true_init_state_priors)[0]\n",
    "    state_seq.append(state)\n",
    "    \n",
    "    while True:\n",
    "        state = np.random.choice(\n",
    "            np.arange(n_states), 1, \n",
    "            p=true_transitions[state])[0]\n",
    "        state_seq.append(state)\n",
    "        if state == n_states - 1 and np.random.rand() > .87:\n",
    "            break\n",
    "    \n",
    "    dataset_y.append(np.array(state_seq))\n",
    "\n",
    "dataset_x = []\n",
    "\n",
    "for y in dataset_y:\n",
    "    dataset_x.append(mu[y] + np.random.randn(len(y), input_size) * 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Create the model and training routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = [\n",
    "    [.84, .16, .00, .00, .00, .00],\n",
    "    [.08, .84, .08, .00, .00, .00],\n",
    "    [.00, .15, .84, .01, .00, .00],\n",
    "    [.00, .00, .00, .84, .16, .00],\n",
    "    [.00, .00, .00, .08, .84, .08],\n",
    "    [.00, .00, .00, .00, .16, .84]]\n",
    "init_state_priors = np.full([n_states], 1 / n_states, dtype='float32')\n",
    "state_priors = np.full([n_states], 1 / n_states, dtype='float32')\n",
    "\n",
    "plot_transition_matrix(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Models and inference\n",
    "\n",
    "training = tf.placeholder(tf.bool, shape=[], name='training')\n",
    "inputs = tf.placeholder(dtype='float32', shape=[None, input_size], name='inputs')\n",
    "\n",
    "net = SkelFeatureExtractor(n_states)\n",
    "\n",
    "logits = net(inputs, training)\n",
    "state_posteriors = tf.nn.log_softmax(logits)\n",
    "\n",
    "hmm = HybridHMMTransitions(transitions, init_state_priors, state_priors)\n",
    "\n",
    "pseudo_lkh = hmm.pseudo_lkh(state_posteriors)\n",
    "ml_state_alignment = hmm.viterbi(pseudo_lkh)\n",
    "\n",
    "# Training\n",
    "\n",
    "state_tgt = tf.placeholder(dtype='int64', shape=[None], name='state_tgt')\n",
    "loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=state_tgt,\n",
    "    logits=logits,\n",
    "    name='cross_entropy'))\n",
    "\n",
    "learning_rate = tf.placeholder(dtype='float32', shape=[], name='learning_rate')\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)  # batch norm updates\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    optimization_op = optimizer.minimize(loss, var_list=net.trainable_variables)\n",
    "\n",
    "add_transition_stats_op = hmm.add_transition_stats(pseudo_lkh)\n",
    "update_transitions_op = hmm.update_transitions()\n",
    "add_prior_stats_op = hmm.add_priors_stats(pseudo_lkh)\n",
    "update_state_priors = hmm.update_state_priors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sess' in globals():\n",
    "    sess.close()\n",
    "    \n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training procedure\n",
    "\n",
    "## First training pass\n",
    "\n",
    "Training begins with the posterior state model, that is to say the Neural Network.\n",
    "In order to proceed, target values are needed.\n",
    "In this case, the targets are the state values which are unfortunately unobserved.\n",
    "To circumvent this problem, we initially select suboptimal values chosen arbitrarily according to an heuristic adapted to the use-case, for example the result of a GMM-HMM model or a k-means clustering.\n",
    "\n",
    "Once all model parts have been trained at least once, the model is assumed to be good enough to provide sensible state values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arbitrary state posterior targets set by heuristic\n",
    "# we assume a uniform succession of states over the sequence\n",
    "state_alignments = [\n",
    "    np.floor(np.linspace(0, n_states - 0.000001, len(x))).astype(np.int32)\n",
    "    for x in dataset_x]\n",
    "\n",
    "# state posterior model training iterations\n",
    "net_losses = []\n",
    "for _ in range(100):\n",
    "    i = np.random.randint(len(dataset_x))\n",
    "    loss_value, _ = sess.run(\n",
    "        [loss, optimization_op],\n",
    "        feed_dict={\n",
    "            inputs: dataset_x[i],\n",
    "            state_tgt: state_alignments[i],\n",
    "            learning_rate: 0.001,\n",
    "            training: True\n",
    "        })\n",
    "    net_losses.append(loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the posterior model, it is possible to train the transition model.\n",
    "The accumulated statistics from all the observation sequences are used to update the initial state prior $\\pi_i = p(s_1=i)$ and the transition probabilities $a_{ij} = p(s_{t+1}=j \\mid s_t=i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dataset_x:\n",
    "    sess.run(\n",
    "        [add_transition_stats_op],\n",
    "        feed_dict={\n",
    "            inputs: x,\n",
    "            training: False\n",
    "        })\n",
    "\n",
    "sess.run(update_transitions_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe how often each state is visited:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_states():\n",
    "    state_alignments = np.concatenate([\n",
    "            sess.run(ml_state_alignment, feed_dict={inputs: x, training: False})\n",
    "            for x in dataset_x])\n",
    "    counts = np.sum(state_alignments[:, None] == np.arange(hmm.n_states)[None, :], axis=0)\n",
    "    \n",
    "    return counts\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(np.arange(hmm.n_states), count_states())\n",
    "plt.title(\"state counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process has led the model to skip over some states in favor of more easily recognizable ones.\n",
    "This is due to the sub-optimal state alignment initially used, the posterior model recognizes some states more easily than others, the latter therefore have a small likelihood which in turn leads the transition model to avoid them.\n",
    "\n",
    "To prevent this degradation, we will force the model to visit some states more. The current state count is obtained under the asumption of uniform state priors, we will force the model to visit them more often by reducing their prior, and therefore increasing their likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heuristic_priors_adjustment(hmm, count_states, sess)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(np.arange(hmm.n_states), count_states())\n",
    "plt.title(\"state counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recaptulate the training process thus far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 2), dpi=100)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(range(len(net_losses)), net_losses, c='red', s=10)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.title(\"CE loss\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plot_transition_matrix(np.exp(sess.run(hmm.A)))\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(np.arange(hmm.n_states), np.exp(sess.run(hmm.state_priors)))\n",
    "plt.title(\"state priors\")\n",
    "plt.ylim((0.01, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are successive refinement iterations of the model parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(5):    \n",
    "    # realign states\n",
    "    state_alignments = [\n",
    "        sess.run(ml_state_alignment, feed_dict={inputs: x, training: False})\n",
    "        for x in dataset_x]\n",
    "\n",
    "    # state posterior model (Neural Network)\n",
    "    for _ in range(100):\n",
    "        i = np.random.randint(len(dataset_x))\n",
    "        loss_value, _ = sess.run(\n",
    "            [loss, optimization_op],\n",
    "            feed_dict={\n",
    "                inputs: dataset_x[i],\n",
    "                state_tgt: state_alignments[i],\n",
    "                learning_rate: 0.0005,\n",
    "                training: True\n",
    "            })\n",
    "        net_losses.append(loss_value)\n",
    "    \n",
    "    # transition model\n",
    "    for x in dataset_x:\n",
    "        sess.run(\n",
    "            add_transition_stats_op,\n",
    "            feed_dict={\n",
    "                inputs: x,\n",
    "                training: False\n",
    "            })\n",
    "\n",
    "    sess.run(update_transitions_op)\n",
    "    \n",
    "    # state priors\n",
    "    for x in dataset_x:\n",
    "        sess.run(\n",
    "            add_prior_stats_op,\n",
    "            feed_dict={\n",
    "                inputs: x,\n",
    "                training: False\n",
    "            })\n",
    "    \n",
    "    sess.run(update_state_priors)\n",
    "    \n",
    "    plt.figure(figsize=(8, 2), dpi=100)\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.scatter(np.arange(len(net_losses) - 100), net_losses[:-100], c='gray', s=10)\n",
    "    plt.scatter(np.arange(len(net_losses) - 100, len(net_losses)), net_losses[-100:], c='red', s=10)\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.title(\"CE loss\")\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plot_transition_matrix(np.exp(sess.run(hmm.A)))\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.bar(np.arange(hmm.n_states), np.exp(sess.run(hmm.state_priors)))\n",
    "    plt.title(\"state priors\")\n",
    "    plt.ylim((0.01, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "for i in range(10):\n",
    "    p, a = sess.run(\n",
    "        [tf.argmax(logits, axis=1), ml_state_alignment],\n",
    "        feed_dict={\n",
    "            inputs: dataset_x[i],\n",
    "            training: False\n",
    "        })\n",
    "    print(\"logits:   \" + ''.join(map(str, p)))\n",
    "    print(\"viterbi:  \" + ''.join(map(str, a)))\n",
    "    print(\"gndtruth: \" + ''.join(map(str, dataset_y[i])))\n",
    "    err_summary = ['✓' if a_ == y_ else '⨯'\n",
    "                   for a_, y_ in zip(class_mapping[a], class_mapping[dataset_y[i]])]\n",
    "    print(\"          \" + ''.join(map(str, err_summary)))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
